# -*- coding: utf-8 -*-
"""retrieve_w/o_prompt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PX366YiPQWNBfNKz4INfQAutZdt2HYQs
"""

!pip install -U pymilvus

!pip install pymilvus['model']

!pip install milvus-lite==2.4.8

import random
import re
import os
import uuid
import pymilvus
from pymilvus import MilvusClient
import milvus_lite
from pymilvus import model
from pymilvus import FieldSchema, DataType, CollectionSchema, Collection, connections, utility
import numpy as np

client = MilvusClient("milvus_demo.db")

folder_path = '/content/drive/MyDrive/text_files'
file_data_dict = {}

for filename in os.listdir(folder_path):
    if filename.endswith('.txt'):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, 'r') as file:
            content = file.read()
            unique_id = int(uuid.uuid4())  # Generate a unique ID
            unique_id = unique_id % 1000000000
            file_data_dict[unique_id] = {
                'filename': filename,
                'content': content
            }

def chunk_text(text, chunk_size):
    """Chunk text into smaller parts of a given size."""
    chunks = []
    while len(text) > chunk_size:
        chunks.append(text[:chunk_size])
        text = text[chunk_size:]
    chunks.append(text)  # Add the last chunk
    return chunks

def recursive_chunk(file_data_dict, chunk_size):
    """Apply chunking recursively to the content of each file in the dictionary."""
    chunked_data = {}
    for unique_id, data in file_data_dict.items():
        content = data['content']
        chunks = chunk_text(content, chunk_size)
        chunked_data[unique_id] = {
            'filename': data['filename'],
            'chunks': chunks
        }
    return chunked_data

chunks=recursive_chunk(file_data_dict, 100)

#chunks=list(chunks.values())
print(chunks)

for unique_id, data in file_data_dict.items():
    print(f"ID: {unique_id}")
    print(f"Filename: {data['filename']}")
    print(f"Content: {data['content']}")

file_ids = list(file_data_dict.keys())
random.shuffle(file_ids)

def assign_files_to_names(file_data_dict, file_ids, names_list):
    """
    Assigns files to names in a round-robin fashion.

    Args:
        file_data_dict (dict): Dictionary where keys are file IDs and values are file data.
        file_ids (list): List of file IDs.
        names_list (list): List of names to assign files to.

    Returns:
        dict: A dictionary where keys are names and values are lists of file data with IDs.
    """
    # Initialize the result dictionary with names as keys and empty lists as values
    names_dict = {name: [] for name in names_list}

    # Assign files to names
    for i, file_id in enumerate(file_ids):
        name = names_list[i % len(names_list)]  # Use modulo to cycle through names
        file_data = file_data_dict[file_id]
        # Format the file data with ID first, then filename, and content last
        formatted_file_data = {
            'id': file_id,
            'filename': file_data['filename'],
            'content': file_data['content']
        }
        names_dict[name].append(formatted_file_data)

    return names_dict

names_list=["Amaan", "Huzan", "Umair", "Waleed"]
names_dict = assign_files_to_names(file_data_dict, file_ids, names_list)
print(names_dict)

def extract_metadata(names_dict):
    """Extract metadata from the names_dict."""
    metadata = {}
    for name, paragraphs in names_dict.items():
        metadata[name] = {
            'name': name,
            'ID': [para['id'] for para in paragraphs], # Access the 'ID' value for each paragraph in the list
            'filename': [para['filename'] for para in paragraphs], # Access the 'text' value for each paragraph in the list
            'content': [para['content'] for para in paragraphs], # Access the 'text' value for each paragraph in the list# Access the 'text' value for each paragraph in the list           'content': [para['content'] for para in paragraphs], # Access the 'text' value for each paragraph in the list
        }
    return metadata

metadata = extract_metadata(names_dict)
for key, value in metadata.items():
    print(f"{key}: {value}\n")

"""!pip install pymilvus[model]"""

embedding_fn = model.DefaultEmbeddingFunction()

vector=embedding_fn.encode_documents(names_dict)

chunks=list(chunks.values())
sub=names_dict.keys()
sub=list(sub)
data = [
    {
        "id": i,
        "vector": vector[i],
        "chunks": chunks[i],
        "meta": {
                "name": metadata[sub[i]]['name'],
                "text_id": metadata[sub[i]]['ID'],
                "filename": metadata[sub[i]]['filename'],
                "content": metadata[sub[i]]['content'],
         }
      }
    for i in range(len(vector))
]

print(data[0])

if client.has_collection(collection_name="milvus_collection"):
    client.drop_collection(collection_name="milvus_collection")
client.create_collection(
    collection_name="milvus_collection",
    dimension=768
)

print("Data has", len(data), "entities, each with fields: ", list(data[0].keys()))
print("Vector dim:", len(data[0]["vector"]))

db_insert = client.insert(collection_name="milvus_collection", data=data)
print(db_insert)

filter_expr = f"id in [1,2]"

#filter_exp = f'JSON_CONTAINS(meta["fields"]["text_id"], "{target_ids_str}")'

# Print filter expression for verification
print("Filter expression:", filter_expr)

# Print filter expression for verification

result = client.query(
    collection_name="milvus_collection",
    filter=filter_expr,
)

print(result)
for i in range(len(result)):
    print(result[i])

