# -*- coding: utf-8 -*-
"""Chunkings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SA-r0KwzAY0QBRBXLweP0zGlK-D1inKu
"""

!pip install transformers sentence-transformers faiss-cpu

!pip install python-docx

!pip install rouge_score

!pip install fuzzywuzzy python-levenshtein

import spacy
import docx
import doc
from collections import Counter
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
from sentence_transformers import SentenceTransformer, util
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
import numpy as np
import re
import faiss
from transformers import pipeline
import nltk
from nltk.tree import Tree
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import sent_tokenize
from rouge_score import rouge_scorer

nlp=spacy.load("en_core_web_sm")

model=SentenceTransformer('all-MiniLM-L6-v2')

# Load data
text_path = "/content/drive/MyDrive/inputArticle.docx"
doc = docx.Document(text_path)
text_data = []
for para in doc.paragraphs:
  if para.text.strip():
    text_data.append(para.text.strip())

#sentences = re.split(r'(?<=[.?!])\s+',text_data)
# Split text into sentences
embeddings = model.encode(text_data)

"""**SEMANTIC CHUNKING(COSINE SIMILARITY)**"""

def create_chunks(sentences, max_chunk_size, similarity_threshold):

    # Convert TF-IDF matrix to dense array (for cosine similarity calculation)

    chunks = []
    curr_chunk = []


    for i, sentence in enumerate(sentences):
        if curr_chunk:
           # Calculate cosine similarity between current sentence and last sentence in current chunk
            similarity = cosine_similarity([embeddings[i]], [curr_chunk[-1]])[0][0]

            # Check similarity and chunk size conditions
            if similarity >= similarity_threshold or len(curr_chunk) >= max_chunk_size:
                chunks.append(' '.join(curr_chunk))
                curr_chunk = []
                curr_chunk_tfidf = []

        # Add current sentence and its TF-IDF vector to current chunk
        curr_chunk.append(sentence)

    # Add the last chunk if it's not empty
    if curr_chunk:
       chunks.append(' '.join(curr_chunk))

    return chunks

"""**CHUNKING USING KMEANS**"""

def chunks_through_kmeans(sentences, k):
  kmeans=KMeans(n_clusters=k, random_state=42)
  kmeans.fit(embeddings)
  cluster=kmeans.labels_
  print(len(cluster))
  chunks=[[]  for _ in range(k)]
  for i, sentence in enumerate(sentences):
      chunks[cluster[i]].append(sentence)

  return chunks

"""### **CHUNKING USING TEXT-TILING**"""

def text_tiling(text, w, k):
    # Use NLTK's TextTilingTokenizer
    tokenizer = nltk.tokenize.texttiling.TextTilingTokenizer(w=w, k=k)
    if isinstance(text, list):
        text = "\n\n".join(text)
    segments = tokenizer.tokenize(text)
    return segments

"""**KEYWORD SEARCH USING FUZZYWUZZY**"""

def custom_keyword_search(query, chunks, top_k):
    def preprocess(text):
        # Convert to lowercase and split into words
        return re.findall(r'\w+', text.lower())

    query_words = set(preprocess(query))

    def score_chunk(chunk):
        chunk_words = preprocess(chunk)
        word_freq = Counter(chunk_words)

        exact_matches = sum(word_freq[word] for word in query_words if word in word_freq)

        # Partial matching
        partial_matches = sum(
            max(word_freq[chunk_word] for chunk_word in word_freq if query_word in chunk_word)
            for query_word in query_words
        )

        # Combine scores, giving more weight to exact matches
        score = (exact_matches * 2 + partial_matches) / len(chunk_words) if chunk_words else 0
        return score

    scored_chunks = [(chunk, score_chunk(chunk)) for chunk in chunks]
    return sorted(scored_chunks, key=lambda x: x[1], reverse=True)[:top_k]

def recursive_chunk(sentences, model, threshold):
    """
    Recursively chunks the given sentences using the provided model and threshold.

    Args:
        sentences: A list of sentences to chunk.
        model: A SentenceTransformer model.
        threshold: A float threshold for cosine similarity.

    Returns:
        A Tree representing the chunked sentences.
    """

    # Base case: If there is only one sentence, return it as a leaf node.
    if len(sentences) == 1:
        return Tree('S', [sentences[0]])

    # Compute pairwise similarities between all sentences.
    embeddings = model.encode(sentences)
    similarities = cosine_similarity(embeddings)

    # Find the pair with the highest similarity.
    np.fill_diagonal(similarities, -1)  # Ignore self-similarities by setting the diagonal to -1
    max_similarity = np.max(similarities)
    max_index = np.argmax(similarities)

    # If the highest similarity is below the threshold, return the sentences as a flat list.
    if max_similarity < threshold:
        return Tree('S', sentences)

    # Otherwise, recursively chunk the two most similar sentences.
    else:
        i, j = np.unravel_index(max_index, similarities.shape)

        # Cast indices to standard Python integers
        i, j = int(i), int(j)

        # Split sentences into two parts: left and right chunks
        left_chunk = sentences[:i+1]
        right_chunk = sentences[i+1:]

        left_tree = recursive_chunk(left_chunk, model, threshold)
        right_tree = recursive_chunk(right_chunk, model, threshold)

        # Combine the left and right chunks
        return Tree('S', [left_tree, right_tree])

def extract_chunks(tree):
    if isinstance(tree, str):
      return [tree]

    chunks = []
    for subtree in tree:
        chunks.extend(extract_chunks(subtree))

    return chunks

threshold = 0.5

#segments = text_tiling(text_data, 20, 10)

chunked_tree = recursive_chunk(text_data, model, threshold)

# Extract chunks from the tree
chunks = extract_chunks(chunked_tree)

chunk_embeddings = model.encode(chunks)
print(f"Number of chunks : {len(chunks)}")
print(f"Shape of chunk embeddings: {chunk_embeddings.shape}")
"""

print(len(segments))
for i, ch in enumerate(segments):
  print("Segment", i+1,": ")
  print(ch)
  print('----------------------------------------------------------------------------')
  print('\n')
"""

chunk_embeddings_np=np.array(chunk_embeddings)

d=chunk_embeddings_np.shape[1]
index=faiss.IndexFlatIP(d)
index.add(chunk_embeddings_np)

pipe = pipeline("text2text-generation", model="MBZUAI/LaMini-Flan-T5-248M")

def query_text(input_query, model, index, chunks, top_k, threshold):
    query_embedding = model.encode([input_query])[0]
    query_embedding = query_embedding.reshape(1, -1)  # Reshape to a single sample with multiple features

    # Search with FAISS index
    distances, indices = index.search(query_embedding, top_k)

    # Filter chunks based on cosine similarity threshold
    relevant_chunks = []
    for distance, idx in zip(distances.flatten(), indices.flatten()):
        similarity = 1 - distance  # Cosine similarity
        if similarity >= threshold:
            relevant_chunks.append(chunks[idx])

    return relevant_chunks

import matplotlib.pyplot as plt
plt.plot(range(len(chunk_embeddings)), chunk_embeddings)
plt.show()

def generate_text_completion(query, closest_segments, pipe, top_k):
     if closest_segments:
        # Use a separator to join segments
        separator = " <SEP> "
        closest_segments_str = separator.join(closest_segments[:top_k])
        max_length = max(len(closest_segments_str), 50)

        # Generate text completion using T5 with closest_segments as input
        input_text = f"Context: {closest_segments_str} Query: {query}"
        result = pipe(input_text, max_length=max_length)

        generated_text = result[0]['generated_text']
     else:
        generated_text = "Warning: No valid input text found."

     return generated_text

query = "what are the benefits of digital transformation"

closest_chunks = query_text(query, model, index , chunks, 20, threshold)
generated_text = generate_text_completion(query, closest_chunks, pipe, 20)
print(f"Input query: {query}\n")
for i, chunk in enumerate(closest_chunks):
    print(f"Closest chunk {i+1}: \n{chunk}\n")
print(f"Generated completion: \n{generated_text}")

